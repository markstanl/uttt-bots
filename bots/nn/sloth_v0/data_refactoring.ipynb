{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Imports",
   "id": "27166f4a278a8a9c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T16:44:40.575780Z",
     "start_time": "2025-03-26T16:44:25.888677Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch as t \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sloth_parse import parse_bytearray_string_to_sloth_mlp, parse_bytearray_string_to_sloth_cnn"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Preprocessing Functions\n",
    "Can ignore all of this if just training"
   ],
   "id": "3cde928d4ac3e8e5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T17:30:21.126745Z",
     "start_time": "2025-03-22T17:30:21.114420Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_datapoint(datapoint: dict[str, any]) -> dict[str, any]:\n",
    "    \"\"\"\n",
    "    Map function for the dataset to convert the data into a format that can be used by the model.\n",
    "    (viz. convert the state from a bytearray string to a tensor)\n",
    "    Args:\n",
    "        datapoint: the datapoint to be processed\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    game_state_tensor = parse_bytearray_string_to_sloth_cnn(datapoint['state'])\n",
    "    \n",
    "    wins = int(datapoint['num_wins'])\n",
    "    losses = int(datapoint['num_losses'])\n",
    "    draws = int(datapoint['num_draws'])\n",
    "    num_games = wins + losses + draws\n",
    "    score = t.tensor(0.0 if num_games == 0 else (wins - losses) / num_games)\n",
    "    # this score function attempts to normalize the score to be between -1 and 1, like an evaluation function should\n",
    "    \n",
    "    return {\n",
    "        'tensor_state': game_state_tensor,\n",
    "        'score': score\n",
    "    }\n",
    "    \n",
    "    "
   ],
   "id": "cac4deec33232582",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T17:30:27.284527Z",
     "start_time": "2025-03-22T17:30:27.277107Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_dataset(split: str) -> Dataset:\n",
    "    if split not in ['train', 'validation', 'test']:\n",
    "        raise ValueError(f\"Invalid split: {split}\")\n",
    "    \n",
    "    dataset = load_dataset('markstanl/u3t', split=split)\n",
    "    return dataset"
   ],
   "id": "34fe4c76ff7ef504",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 4,
   "source": [
    "train = get_dataset('train')\n",
    "test = get_dataset('test')\n",
    "val = get_dataset('validation')"
   ],
   "id": "fe0b91ebe128c1ec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T17:30:43.321693Z",
     "start_time": "2025-03-22T17:30:43.309745Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_tensor_dataset(dataset: Dataset) -> Dataset:\n",
    "    dataset = dataset.remove_columns(['num_visits', 'actions', 'depth'])\n",
    "    \n",
    "    dataset = dataset.map(process_datapoint)\n",
    "    dataset.set_format(type='torch', columns=['tensor_state'])\n",
    "        \n",
    "    dataset = dataset.remove_columns(['state', 'num_wins', 'num_losses', 'num_draws'])\n",
    "    \n",
    "    return dataset\n",
    "    \n",
    "def save_tensor_dataset(dataset: Dataset, split: str):\n",
    "    dataset.to_parquet(f'sloth_{split}.parquet')\n",
    "    \n",
    "def make_tensor_dataset(dataset: Dataset, split: str):\n",
    "    tensor_dataset = get_tensor_dataset(dataset)\n",
    "    save_tensor_dataset(tensor_dataset, split)"
   ],
   "id": "f650128398c68f6b",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T19:40:46.798757Z",
     "start_time": "2025-03-22T17:31:12.488817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "make_tensor_dataset(train, 'train')\n",
    "make_tensor_dataset(test, 'test')\n",
    "make_tensor_dataset(val, 'validation')"
   ],
   "id": "4f0ef02d96c953a8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/5601458 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "99a229c1b5ee4e5fa6a3c8553fed7579"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/5602 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b92d548211d14746bf88bbdefabeaabc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1600367 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "77241d6106ef4fa7a199d55e0dd117c1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1601 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5c36734b6e8643bbac85a98fee0eeb2d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/800165 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "caa4b55959ed44858687afe212fae972"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/801 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "da46b31c50de4ae5b8523a0baf84752b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Though, run this block to get the data loader from your local dataset",
   "id": "7e8e93e24ccbca86"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T16:46:08.841384Z",
     "start_time": "2025-03-26T16:46:08.830811Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_local_data_loader(split: str, cnn: bool=False) -> t.utils.data.DataLoader:\n",
    "    \"\"\"\n",
    "    Get a DataLoader for the given split of the data. Converts the data to tensors.\n",
    "    Args:\n",
    "        split: the split of the data to get the DataLoader for\n",
    "        cnn: boolean to use the cnn split\n",
    "\n",
    "    Returns:\n",
    "        DataLoader: the DataLoader for the given split\n",
    "    \"\"\"\n",
    "    if cnn:\n",
    "        dataset = Dataset.from_parquet(f'sloth_data/sloth_{split}_cnn.parquet')\n",
    "    else:\n",
    "        dataset = Dataset.from_parquet(f'sloth_data/sloth_{split}.parquet')\n",
    "    dataset.set_format(type='torch', columns=['tensor_state', 'score'])\n",
    "    \n",
    "    loader = t.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    return loader\n",
    "\n",
    "def get_loader_from_dataset(dataset: Dataset) -> t.utils.data.DataLoader:\n",
    "    dataset.set_format(type='torch', columns=['tensor_state', 'score'])\n",
    "    \n",
    "    loader = t.utils.data.DataLoader(dataset, batch_size=256, shuffle=True)\n",
    "    return loader\n",
    "\n",
    "def get_local_dataset(split: str) -> Dataset:\n",
    "    return Dataset.from_parquet(f'sloth_data/sloth_{split}_cnn.parquet')"
   ],
   "id": "3c26c50955790435",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Understanding Data Conversion\n",
    "I am having trouble with the tensors in the dataset. I realized the issue was with loading datasets as tensors from the parquet file, the solution is to set the format of the dataset to torch when loading."
   ],
   "id": "cdbd96a8412cad21"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2a3b116f5daf4cebb25985e8ab32c16f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'state': string\n",
      "Column 'num_wins': int64\n",
      "Column 'num_draws': int64\n",
      "Column 'num_losses': int64\n",
      "Column 'tensor_state': list\n",
      "Column 'score': float32\n"
     ]
    }
   ],
   "execution_count": 131,
   "source": [
    "train_test = train[:10]\n",
    "train_test_dataset = Dataset.from_dict(train_test)\n",
    "\n",
    "tensor_dataset = get_tensor_dataset(train_test_dataset)"
   ],
   "id": "e6e4844fca94dc2c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0a7a3fe87e0a433ba4871f2dcbd24397"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7b236f570d36491180aae7a0c546b2a6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 136,
   "source": [
    "make_tensor_dataset(train_test_dataset, 'train_test')\n",
    "loader = get_local_data_loader('train_test')"
   ],
   "id": "c4df577d0475b985"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 137,
   "source": "batch = next(iter(loader))",
   "id": "96c09801d7031029"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "execution_count": 138,
   "source": [
    "example = batch['tensor_state'][0]\n",
    "print(type(example))"
   ],
   "id": "187224941e98defb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 81])\n"
     ]
    }
   ],
   "execution_count": 140,
   "source": "print(example.shape)",
   "id": "3157f82d0925ba9d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 1., 1.,  ..., 1., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 1.,  ..., 1., 1., 1.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 1.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]]) torch.Size([10, 4, 81])\n",
      "tensor([[1., 0., 0.,  ..., 1., 1., 1.],\n",
      "        [1., 0., 0.,  ..., 1., 1., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 1., 1., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]) torch.Size([10, 324])\n",
      "tensor([[-0.0249],\n",
      "        [-0.0343],\n",
      "        [-0.0330],\n",
      "        [-0.0233],\n",
      "        [-0.0146],\n",
      "        [-0.0203],\n",
      "        [-0.0236],\n",
      "        [-0.0324],\n",
      "        [-0.0169],\n",
      "        [-0.0175]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "execution_count": 149,
   "source": [
    "tensors = batch['tensor_state']\n",
    "print(model(tensors))"
   ],
   "id": "e81df9a495a70af9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluation",
   "id": "449403f2c4edeb68"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "test_loader = get_local_data_loader('test')",
   "id": "468b049fedb8f5c5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "evaluate(model, test_loader)",
   "id": "12949e2aa7a3032f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Visualization",
   "id": "426be0494a766ed4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T16:36:17.762978Z",
     "start_time": "2025-03-20T16:36:14.619991Z"
    }
   },
   "cell_type": "code",
   "source": "test_set = get_local_dataset('test')\n",
   "id": "bf8961d00abfaf3e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['tensor_state', 'score'],\n",
      "    num_rows: 1600367\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T16:36:40.276615Z",
     "start_time": "2025-03-20T16:36:38.178443Z"
    }
   },
   "cell_type": "code",
   "source": "scores = test_set['score']",
   "id": "32e01baf42f62891",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T16:41:06.665843Z",
     "start_time": "2025-03-20T16:41:06.660990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ],
   "id": "cb393ceca1552bea",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T01:32:03.057498Z",
     "start_time": "2025-03-25T01:32:03.030340Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(scores)\n",
    "counts, bins = np.histogram(scores)\n",
    "plt.stairs(counts, bins)\n",
    "plt.show()\n"
   ],
   "id": "5be1f71b70c9f4b3",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mscores\u001B[49m)\n\u001B[0;32m      2\u001B[0m counts, bins \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mhistogram(scores)\n\u001B[0;32m      3\u001B[0m plt\u001B[38;5;241m.\u001B[39mstairs(counts, bins)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'scores' is not defined"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T01:33:12.895183Z",
     "start_time": "2025-03-25T01:33:01.430527Z"
    }
   },
   "cell_type": "code",
   "source": "train_set_test = get_local_dataset('train')",
   "id": "afb9244d92bd36cb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/17 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "73cada7d217241e9ad8c5ea52980b40e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T03:58:57.319786Z",
     "start_time": "2025-03-25T03:51:33.741287Z"
    }
   },
   "cell_type": "code",
   "source": "hf_set = load_dataset('markstanl/u3t', data_dir='data/state_eval', split='train')",
   "id": "fd485e5b0435988c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "README.md:   0%|          | 0.00/9.25k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cec5b3fc0ef042cfaf5705e62137bdca"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f3a2c042fc084232aafc656afbf444db"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "03a6fa5ac37a403593999b69d6b35bd3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6e5c0a5b7cd148f9b4b60dcf50c7e0cd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/17 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a3ef3d77df2e43a294b601e8f5f112d4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T01:42:39.051429Z",
     "start_time": "2025-03-25T01:42:39.045096Z"
    }
   },
   "cell_type": "code",
   "source": "print(train_set_test)",
   "id": "1d4846671b4d625e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['tensor_state', 'score'],\n",
      "    num_rows: 5601458\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T02:01:18.901580Z",
     "start_time": "2025-03-25T02:01:18.892937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(train_set_test)\n",
    "print(train_set_test.features.items())"
   ],
   "id": "6891baceaa316c8f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['tensor_state', 'score'],\n",
      "    num_rows: 5601458\n",
      "})\n",
      "dict_items([('tensor_state', Sequence(feature=Sequence(feature=Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), length=-1, id=None), length=-1, id=None)), ('score', Value(dtype='float32', id=None))])\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7fe869446d8ce561"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
