{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm"
   ],
   "id": "309f653b81ee0828"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T02:39:39.147176Z",
     "start_time": "2025-03-28T02:39:38.527539Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_dataset(split: str) -> Dataset:\n",
    "    \"\"\"\n",
    "    Loads the tensor dataset from hugging face\n",
    "    \"\"\"\n",
    "    if split not in ['train', 'validation', 'test']:\n",
    "        raise ValueError(f\"Invalid split: {split}\")\n",
    "\n",
    "    dataset = load_dataset('markstanl/u3t', data_dir='data/state_eval', split=split)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_loader(split: str, batch_size: int = 256) -> t.utils.data.DataLoader:\n",
    "    \"\"\"\n",
    "    Returns a DataLoader for the given split\n",
    "    \"\"\"\n",
    "    dataset = get_dataset(split)\n",
    "    return t.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "def get_loader_gpu(split: str,\n",
    "                   batch_size: int = 256,\n",
    "                   num_workers: int = 2,\n",
    "                   pin_memory: bool = True) -> t.utils.data.DataLoader:\n",
    "    \"\"\"\n",
    "    Returns a DataLoader for the given split\n",
    "    \"\"\"\n",
    "    dataset = get_dataset(split)\n",
    "    return t.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers,\n",
    "                                   pin_memory=pin_memory)"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mget_dataset\u001B[39m(split: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[43mDataset\u001B[49m:\n\u001B[0;32m      2\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\"\"\"\u001B[39;00m\n\u001B[0;32m      3\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m split \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvalidation\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m'\u001B[39m]:\n",
      "\u001B[1;31mNameError\u001B[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def evaluate(model: nn.Module,\n",
    "             loader: t.utils.data.DataLoader,\n",
    "             criterion: t.nn.modules.loss,\n",
    "             num_batches: int = 100):\n",
    "    model.eval()\n",
    "    with t.no_grad():\n",
    "        total_loss = 0\n",
    "        total_error = 0\n",
    "        num_data_points = 0\n",
    "\n",
    "        for i, batch in enumerate(loader):\n",
    "            if i >= num_batches:\n",
    "                break\n",
    "            state, score = batch['tensor_state'], batch['score']\n",
    "            output = model(state)\n",
    "            loss = criterion(output, score.unsqueeze(1))\n",
    "\n",
    "            num_data_points += len(score)\n",
    "            total_loss += loss.item()\n",
    "            total_error += t.sum(t.abs(output - score.unsqueeze(1))).item()\n",
    "        loss = total_loss / num_data_points\n",
    "        error = total_error / num_data_points\n",
    "\n",
    "        return loss, error\n",
    "\n",
    "\n",
    "def train(model: nn.Module,\n",
    "          loader: t.utils.data.DataLoader,\n",
    "          optimizer: t.optim,\n",
    "          criterion: t.nn.modules.loss,\n",
    "          num_epochs: int,\n",
    "          test_loader: t.utils.data.DataLoader,\n",
    "          epoch_start: int = 0,\n",
    "          path_name: str = None) -> list[tuple[float, float, float, float]]:\n",
    "    \"\"\"\n",
    "    Train the model on the given data\n",
    "    Args:\n",
    "        model: the model to train\n",
    "        loader: the DataLoader for the data\n",
    "        optimizer: the optimizer to use\n",
    "        criterion: the loss function to use\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    history = []\n",
    "    save_path = path_name if path_name else \"sloth_models/model_cnn_epoch_{epoch}.pth\"\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        total_error = 0\n",
    "        num_data_points = 0\n",
    "\n",
    "        for i, batch in tqdm(enumerate(loader)):\n",
    "            optimizer.zero_grad()\n",
    "            state, score = batch['tensor_state'], batch['score']\n",
    "            output = model(state)\n",
    "            loss = criterion(output, score.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            num_data_points += len(score)\n",
    "            total_loss += loss.item()\n",
    "            total_error += t.sum(t.abs(output - score.unsqueeze(1))).item()\n",
    "\n",
    "        loss = total_loss / num_data_points\n",
    "        error = total_error / num_data_points\n",
    "\n",
    "        if test_loader is None:\n",
    "            print(f\"Epoch {epoch + epoch_start} Loss: {loss} Average Error {error}\")\n",
    "            history.append((loss, error))\n",
    "        else:\n",
    "            test_loss, test_error = evaluate(model, test_loader, criterion, num_batches=100)\n",
    "            print(\n",
    "                f\"Epoch {epoch + epoch_start} Loss: {loss} Average Error {error} Test Loss: {test_loss} Test Error: {test_error}\")\n",
    "            history.append((loss, error, test_loss, test_error))\n",
    "\n",
    "        try:\n",
    "            t.save({\n",
    "                'epoch': epoch + epoch_start + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "            }, save_path.format(epoch=epoch + epoch_start + 1))\n",
    "        except Exception:\n",
    "            t.save({\n",
    "                'epoch': epoch + epoch_start + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "            }, \"sloth_models/backup_model_{epoch}.pth\".format(epoch=epoch + 10))\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "def train_gpu(model: nn.Module,\n",
    "              loader: t.utils.data.DataLoader,\n",
    "              optimizer: t.optim,\n",
    "              criterion: t.nn.modules.loss,\n",
    "              device: t.device,\n",
    "              num_epochs: int,\n",
    "              test_loader: t.utils.data.DataLoader,\n",
    "              epoch_start: int = 0,\n",
    "              path_name: str = None) -> list[tuple[float, float, float, float]]:\n",
    "    \"\"\"\n",
    "    Train the model on the given data\n",
    "    Args:\n",
    "        model: the model to train\n",
    "        loader: the DataLoader for the data\n",
    "        optimizer: the optimizer to use\n",
    "        criterion: the loss function to use\n",
    "        device: the device to train on\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    scaler = t.cuda.amp.GradScaler()\n",
    "    history = []\n",
    "\n",
    "    save_path = path_name if path_name else \"sloth_models/model_cnn_epoch_{epoch}.pth\"\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        total_error = 0\n",
    "        num_data_points = 0\n",
    "\n",
    "        for batch in tqdm(loader):\n",
    "            state = batch['tensor_state'].to(device, non_blocking=True, memory_format=t.channels_last)\n",
    "            score = batch['score'].to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with t.cuda.amp.autocast():\n",
    "                output = model(state)\n",
    "                loss = criterion(output, score.unsqueeze(1))\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            num_data_points += len(score)\n",
    "            total_loss += loss.item()\n",
    "            total_error += t.sum(t.abs(output - score.unsqueeze(1))).item()\n",
    "\n",
    "        loss = total_loss / num_data_points\n",
    "        error = total_error / num_data_points\n",
    "\n",
    "        if test_loader is None:\n",
    "            print(f\"Epoch {epoch + epoch_start} Loss: {loss} Average Error {error}\")\n",
    "            history.append((loss, error))\n",
    "        else:\n",
    "            test_loss, test_error = evaluate(model, test_loader, criterion, num_batches=100)\n",
    "            print(\n",
    "                f\"Epoch {epoch + epoch_start} Loss: {loss} Average Error {error} Test Loss: {test_loss} Test Error: {test_error}\")\n",
    "            history.append((loss, error, test_loss, test_error))\n",
    "\n",
    "        try:\n",
    "            t.save({\n",
    "                'epoch': epoch + epoch_start + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "            }, save_path.format(epoch=epoch + epoch_start + 1))\n",
    "        except Exception:\n",
    "            t.save({\n",
    "                'epoch': epoch + epoch_start + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "            }, \"sloth_models/backup_model_{epoch}.pth\".format(epoch=epoch + 10))\n",
    "\n",
    "    return history"
   ],
   "id": "4a3a85a5c34067fb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Models",
   "id": "e647ed2efb822507"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T02:42:56.202874Z",
     "start_time": "2025-03-28T02:42:56.041586Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple MLP for the U3T dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(324, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        return self.linear_relu_stack(x)\n",
    "\n",
    "\n",
    "class CNN_small(nn.Module):\n",
    "    \"\"\"\n",
    "    A CNN model with a lower stride.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CNN_small, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "        self.fc1 = nn.Linear(256 * 9 * 9, 512)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.relu6 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.relu4(x)\n",
    "\n",
    "        x = t.flatten(x, start_dim=1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu5(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu6(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Deep_CNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A deep CNN model for the U3T dataset. The stride is 3, the argument being that this ensures the kernel only sees\n",
    "    the subgames, and not any of the overlap. Except for the next one, which has a stride of 1 because the convolution\n",
    "    makes the 9x9 into a 3x3 board, in which seeing the overlap may be valuable.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Deep_CNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(4, 128, kernel_size=3, stride=3, padding=0)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(128, 256, kernel_size=2, stride=1, padding=0)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.fc1 = nn.Linear(256, 512)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.relu6 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "\n",
    "        x = t.flatten(x, start_dim=1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu5(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu6(x)\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        return x\n"
   ],
   "id": "3c5fa9a22dcd59a0",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mMLP\u001B[39;00m(\u001B[43mnn\u001B[49m\u001B[38;5;241m.\u001B[39mModule):\n\u001B[0;32m      2\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m      3\u001B[0m         \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n",
      "\u001B[1;31mNameError\u001B[0m: name 'nn' is not defined"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train_on_cpu(model: nn.Module,\n",
    "                 num_epochs: int,\n",
    "                 batch_size: int = 256,\n",
    "                 learning_rate: float = 0.01,\n",
    "                 ) -> list[tuple[float, float, float, float]]:\n",
    "    \"\"\"\n",
    "    Function to train a given model on the cpu\n",
    "    \"\"\"\n",
    "    train_loader = get_loader('train', batch_size)\n",
    "    test_loader = get_loader('validation', batch_size)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    return train(model, train_loader, optimizer, criterion, num_epochs, test_loader)\n",
    "\n",
    "\n",
    "def train_on_gpu(model: nn.Module,\n",
    "                 num_epochs: int,\n",
    "                 batch_size: int = 256,\n",
    "                 learning_rate: float = 0.01,\n",
    "                 num_workers: int = 2,\n",
    "                 pin_memory: bool = True,\n",
    "                 ) -> list[tuple[float, float, float, float]]:\n",
    "    \"\"\"\n",
    "    Function to train a given model on the cpu\n",
    "    \"\"\"\n",
    "    device = t.device('cuda' if t.cuda.is_available() else 'cpu')\n",
    "    if device == 'cpu':\n",
    "        raise ValueError(\"No GPU available. Dial in.\")\n",
    "\n",
    "    train_loader = get_loader_gpu('train', batch_size, num_workers, pin_memory)\n",
    "    test_loader = get_loader_gpu('validation', batch_size, num_workers, pin_memory)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    return train_gpu(model, train_loader, optimizer, criterion, device, num_epochs, test_loader)\n"
   ],
   "id": "3d852a9aa8e90f39"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8d548d7861b5abaa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
